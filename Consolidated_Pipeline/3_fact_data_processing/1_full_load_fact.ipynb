{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24ae2b61-eb9c-4beb-8b77-29450ef0d87a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abed941c-1705-42b6-8ce8-9b2ed5d55721",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/lavouver96@gmail.com/Consolidated_Pipeline/1_setup/utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d957ce15-6b67-463d-b3e7-bdb9123585e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create the widgets for user input\n",
    "dbutils.widgets.text(\"catalog\", \"fmcg\", \"Catalog\")\n",
    "dbutils.widgets.text(\"data_source\", \"orders\", \"Data Source\")\n",
    "\n",
    "# get the catalog and data source\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "data_source = dbutils.widgets.get(\"data_source\")\n",
    "\n",
    "base_path = f's3://sports-db-side-project/{data_source}'\n",
    "landing_path = f\"{base_path}/landing/\"\n",
    "processed_path = f\"{base_path}/processed/\"\n",
    "print(\"Base Path: \", base_path)\n",
    "print(\"Landing Path: \", landing_path)\n",
    "print(\"Processed Path: \", processed_path)\n",
    "\n",
    "\n",
    "# define the tables\n",
    "bronze_table = f\"{catalog}.{bronze_schema}.{data_source}\"\n",
    "silver_table = f\"{catalog}.{silver_schema}.{data_source}\"\n",
    "gold_table = f\"{catalog}.{gold_schema}.sb_fact_{data_source}\"\n",
    "print(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa5e093c-ee80-48d4-be1b-a6c7b8852705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Bronze Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80385a82-c57d-412e-a109-8766b043f9b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.options(header=True, inferSchema=True) \\\n",
    "        .csv(f\"{landing_path}/*.csv\") \\\n",
    "        .withColumn(\"read_timestamp\", F.current_timestamp()) \\\n",
    "        .select(\"*\", \"_metadata.file_name\", \"_metadata.file_size\")\n",
    "\n",
    "print(\"Total Rows: \", df.count())\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e09bff9a-e253-45fe-87e8-f36e87b36f78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write\\\n",
    " .format(\"delta\") \\\n",
    " .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    " .mode(\"append\") \\\n",
    " .saveAsTable(bronze_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c4e2554-1d88-404a-a384-c15f40e94cd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Moving files from source to processed directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4a858ee-afee-417c-a49e-8818c6886e24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# move the files from landing into precessed\n",
    "files = dbutils.fs.ls(landing_path)\n",
    "for file_info in files:\n",
    "    dbutils.fs.mv(\n",
    "        file_info.path,\n",
    "        f\"{processed_path}/{file_info.name}\",\n",
    "        True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e156a6c1-f500-4e4b-8592-0b6f0b82a7c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Silver Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3762577-f8b8-4635-9dbc-3a7d0ac45abd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_orders = spark.sql(f\"SELECT * FROM {bronze_table}\")\n",
    "df_orders.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8de42054-b18e-4cf4-9ec5-f51f710d1518",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **Transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9da4598e-f2ea-4ac9-a8fd-834b3c11796a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. keep only rows where order_qty is present\n",
    "df_orders = df_orders.filter(F.col(\"order_qty\").isNotNull())\n",
    "\n",
    "\n",
    "# 2. clean customer_id → keep numeric, else set to 999999\n",
    "df_orders = df_orders.withColumn(\n",
    "    \"customer_id\",\n",
    "    F.when(F.col(\"customer_id\").rlike(\"^[0-9]+$\"), F.col(\"customer_id\"))\n",
    "     .otherwise(\"999999\")\n",
    "     .cast(\"string\")\n",
    ")\n",
    "\n",
    "# 3. remove weekday name from the date text\n",
    "#    \"Tuesday, July 01, 2025\" → \"July 01, 2025\"\n",
    "df_orders = df_orders.withColumn(\n",
    "    \"order_placement_date\",\n",
    "    F.regexp_replace(F.col(\"order_placement_date\"), r\"^[A-Za-z]+,\\s*\", \"\")\n",
    ")\n",
    "\n",
    "# 4. parse order_placement_date using multiple possible formats\n",
    "df_orders = df_orders.withColumn(\n",
    "    \"order_placement_date\",\n",
    "    F.coalesce(\n",
    "        F.try_to_date(\"order_placement_date\", \"yyyy/MM/dd\"),\n",
    "        F.try_to_date(\"order_placement_date\", \"dd-MM-yyyy\"),\n",
    "        F.try_to_date(\"order_placement_date\", \"dd/MM/yyyy\"),\n",
    "        F.try_to_date(\"order_placement_date\", \"MMMM dd, yyyy\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5. drop duplicates\n",
    "df_orders = df_orders.dropDuplicates([\"order_id\", \"order_placement_date\", \"customer_id\", \"product_id\", \"order_qty\"])\n",
    "\n",
    "# 5. convert product id to string\n",
    "df_orders = df_orders.withColumn('product_id', F.col('product_id').cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80f8fc3b-b532-468d-ab71-02260b733bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check what's the maximum and minimum date\n",
    "df_orders.agg(\n",
    "    F.min(\"order_placement_date\").alias(\"min_date\"),\n",
    "    F.max(\"order_placement_date\").alias(\"max_date\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20315319-ef58-4f37-8749-76114aa02713",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **Join with products**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e40abe74-c72a-4e3b-ae10-ae8fe4467dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# retrieve the product_code from silver.products\n",
    "df_products = spark.table(\"fmcg.silver.products\")\n",
    "df_joined = df_orders.join(df_products, on=\"product_id\", how=\"inner\").select(df_orders[\"*\"], df_products[\"product_code\"])\n",
    "\n",
    "df_joined.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c637338-35fb-4cd3-a55c-2d04775cebb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if the table doesn't exist, create it, otherwise merge\n",
    "if not (spark.catalog.tableExists(silver_table)):\n",
    "    df_joined.write.format(\"delta\").option(\n",
    "        \"delta.enableChangeDataFeed\", \"true\"\n",
    "    ).option(\"mergeSchema\", \"true\").mode(\"overwrite\").saveAsTable(silver_table)\n",
    "else:\n",
    "    silver_delta = DeltaTable.forName(spark, silver_table)\n",
    "    silver_delta.alias(\"silver\").merge(df_joined.alias(\"bronze\"), \"silver.order_placement_date = bronze.order_placement_date AND silver.order_id = bronze.order_id AND silver.product_code = bronze.product_code AND silver.customer_id = bronze.customer_id\").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10b659be-e3a1-4ad4-b52a-0ec42f094b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Gold Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "427dad52-a253-498d-8d90-2f15ff6dd245",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# only retreive the columns we need for gold layer\n",
    "df_gold = spark.sql(f\"SELECT order_id, order_placement_date as date, customer_id as customer_code, product_code, product_id, order_qty as sold_quantity FROM {silver_table};\")\n",
    "\n",
    "df_gold.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12fec240-8914-4d11-8e5a-37b2a98465f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if the table doesn't exist, create it, otherwise merge\n",
    "if not (spark.catalog.tableExists(gold_table)):\n",
    "    print(\"creating New Table\")\n",
    "    df_gold.write.format(\"delta\").option(\n",
    "        \"delta.enableChangeDataFeed\", \"true\"\n",
    "    ).option(\"mergeSchema\", \"true\").mode(\"overwrite\").saveAsTable(gold_table)\n",
    "else:\n",
    "    gold_delta = DeltaTable.forName(spark, gold_table)\n",
    "    gold_delta.alias(\"source\").merge(df_gold.alias(\"gold\"), \"source.date = gold.date AND source.order_id = gold.order_id AND source.product_code = gold.product_code AND source.customer_code = gold.customer_code\").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a68d2ec6-c00c-4b9b-a559-ed16ea1c741c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Merging with Parent company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57df0ede-28bc-4b9b-b8e4-eb4a32742cb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Note: We want data for monthly level but child data is on daily level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cd1e18d-5437-40ac-bb44-24df4973e99c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### **Full Load**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eed8c007-2879-4bc0-aabe-c342780a7de8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_child = spark.sql(f\"SELECT date, product_code, customer_code, sold_quantity FROM {gold_table}\")\n",
    "df_child.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdcbce41-663c-44f2-850d-56e6e36841bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_child.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d29bcca-6939-4b9b-af41-d6d08ca30d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let it group by month\n",
    "df_monthly = (\n",
    "    df_child\n",
    "    # 1. get month start date (e.g., 2025-11-30 → 2025-11-01)\n",
    "    .withColumn(\"month_start\", F.trunc(\"date\", \"MM\"))   # or F.date_trunc(\"month\", \"date\").cast(\"date\")\n",
    "\n",
    "    # 2. group at monthly grain by month_start + product_code + customer_code\n",
    "    .groupBy(\"month_start\", \"product_code\", \"customer_code\")\n",
    "    .agg(\n",
    "        F.sum(\"sold_quantity\").alias(\"sold_quantity\")\n",
    "    )\n",
    "\n",
    "    # 3. rename month_start back to `date` to match your target schema\n",
    "    .withColumnRenamed(\"month_start\", \"date\")\n",
    ")\n",
    "\n",
    "df_monthly.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "730beffe-21e0-49aa-bf4a-eac7c485b27e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_monthly.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7a59470-17c6-4373-809e-a7eef386eacf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_parent_delta = DeltaTable.forName(spark, f\"{catalog}.{gold_schema}.fact_orders\")\n",
    "\n",
    "gold_parent_delta.alias(\"parent_gold\") \\\n",
    "    .merge(df_monthly.alias(\"child_gold\"), \"parent_gold.date = child_gold.date AND parent_gold.product_code = child_gold.product_code AND parent_gold.customer_code = child_gold.customer_code\") \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1_full_load_fact",
   "widgets": {
    "catalog": {
     "currentValue": "fmcg",
     "nuid": "2c122c07-f469-419f-9de6-26b3b835e579",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "fmcg",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "fmcg",
      "label": "Catalog",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "data_source": {
     "currentValue": "orders",
     "nuid": "5c1b7f2e-7007-427e-8b93-44389c71b385",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "orders",
      "label": "Data Source",
      "name": "data_source",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "orders",
      "label": "Data Source",
      "name": "data_source",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
